{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basic Rag",
   "id": "6cd217f563d086dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:32:05.427470Z",
     "start_time": "2025-08-12T07:32:05.422083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "a51c49540b0b1802",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:32:06.626389Z",
     "start_time": "2025-08-12T07:32:05.843476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"Recommendation System Overview.pdf\"]\n",
    ").load_data()"
   ],
   "id": "a112af631cbf9f8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:32:06.634198Z",
     "start_time": "2025-08-12T07:32:06.632035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ],
   "id": "f1f960868572878e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "69 \n",
      "\n",
      "<class 'llama_index.core.schema.Document'>\n",
      "Doc ID: 5f19857b-c71a-4a4e-b764-178c1d82979c\n",
      "Text: Recommendation System Overview The recommendation system is\n",
      "responsible for generating an optimal work plan for a barge project.\n",
      "It takes project information (tasks, schedules, workers) and produces\n",
      "a Recommended Plan – a schedule of tasks with assigned workers and\n",
      "timing that aims to meet project goals efficiently. The system also\n",
      "computes supp...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:32:08.023527Z",
     "start_time": "2025-08-12T07:32:08.020285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ],
   "id": "f7750c461917bfa0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:32:14.769629Z",
     "start_time": "2025-08-12T07:32:08.821113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "id": "a9e42582fc97f1cd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index = VectorStoreIndex.from_documents([document])\n",
    "query_engine = index.as_query_engine()"
   ],
   "id": "b7b9176475839514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = query_engine.query(\"What main components of system\")\n",
    "print(str(response))"
   ],
   "id": "4a6f9a6f8c3e1c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentence Window",
   "id": "11befa61f52be2d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Document,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "import os\n"
   ],
   "id": "7dc0906457fc55e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "id": "265ca6ba1f73a23d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_sentence_window_index(\n",
    "    document: Document,\n",
    "    save_dir: str = \"sentence_index\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a VectorStoreIndex with a sentence window node parser.\n",
    "\n",
    "    This function will create and save an index if it doesn't exist,\n",
    "    or load it from disk if it does.\n",
    "\n",
    "    Args:\n",
    "        document (Document): The document to index.\n",
    "        save_dir (str): The directory to save/load the index from.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The created or loaded index.\n",
    "    \"\"\"\n",
    "    # Create the sentence window node parser with specified settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=3,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "\n",
    "    # Check if the storage directory already exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        print(f\"Building new index and saving to {save_dir}\")\n",
    "        # If it doesn't exist, create the index from the document\n",
    "        # The node_parser is passed directly to the index constructor.\n",
    "        # The LLM and embed_model are taken from the global Settings.\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            [document],\n",
    "            node_parser=node_parser,\n",
    "        )\n",
    "        # Persist the index to disk\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        print(f\"Loading existing index from {save_dir}\")\n",
    "        # If the directory exists, load the index from storage\n",
    "        # The StorageContext is created from the directory\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=save_dir)\n",
    "        sentence_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    return sentence_index\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index: VectorStoreIndex,\n",
    "    similarity_top_k: int = 6,\n",
    "    rerank_top_n: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a query engine from a sentence window index with postprocessors.\n",
    "\n",
    "    Args:\n",
    "        sentence_index (VectorStoreIndex): The index to query.\n",
    "        similarity_top_k (int): The number of top similar results to retrieve.\n",
    "        rerank_top_n (int): The number of results to return after reranking.\n",
    "\n",
    "    Returns:\n",
    "        The query engine.\n",
    "    \"\"\"\n",
    "    # Define postprocessors to enhance retrieval\n",
    "    # Replaces the metadata with the actual text window\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "\n",
    "    # Reranks the results for better relevance using a free, local model.\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    # Create the query engine from the index\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        node_postprocessors=[postproc, rerank],\n",
    "    )\n",
    "    return sentence_window_engine\n",
    "\n"
   ],
   "id": "1888b60aaeded7fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index = build_sentence_window_index(document, save_dir=\"my_sentence_index\")\n",
    "\n",
    "# Create the query engine\n",
    "query_engine = get_sentence_window_query_engine(index)"
   ],
   "id": "69bf83ce9d89a89c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "response = query_engine.query(\"What is the primary purpose of the QuantitiesRecommendationService, and how does its output directly influence the final schedule generated by the main RecommendationService?\")",
   "id": "81db00bf036d3772",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Source Nodes ---\")\n",
    "for node in response.source_nodes:\n",
    "    # Note: The score from SentenceTransformerRerank is a relevance score, not a similarity score.\n",
    "    # Higher is better.\n",
    "    print(f\"Relevance Score: {node.score:.4f}\")\n",
    "    print(f\"Text: {node.text}\\n\")"
   ],
   "id": "7bb70e5c52bec1b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Auto Merging",
   "id": "db482e4a26752a17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Integration-specific imports\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"Recommendation System Overview.pdf\"]\n",
    ").load_data()\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\", temperature=0.1)\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n"
   ],
   "id": "45b6a171caa8033e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds an auto-merging index from a list of documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list or Document): A list of LlamaIndex Document objects or a single Document.\n",
    "        llm (LLM): The language model to use.\n",
    "        embed_model_name (str): The name of the HuggingFace embedding model.\n",
    "        save_dir (str): Directory to save or load the index from.\n",
    "        chunk_sizes (list): A list of chunk sizes for hierarchical parsing.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The created or loaded auto-merging index.\n",
    "    \"\"\"\n",
    "    # Set up the global settings\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "\n",
    "    if isinstance(documents, Document):\n",
    "        documents = [documents]\n",
    "\n",
    "    # Check if the index already exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        # Build the index from scratch\n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "        storage_context = StorageContext.from_defaults()\n",
    "        storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        # Load the index from storage\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=save_dir)\n",
    "        automerging_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an auto-merging query engine from an index.\n",
    "\n",
    "    Args:\n",
    "        automerging_index (VectorStoreIndex): The auto-merging index.\n",
    "        similarity_top_k (int): The number of similar nodes to retrieve.\n",
    "        rerank_top_n (int): The number of nodes to return after reranking.\n",
    "\n",
    "    Returns:\n",
    "        RetrieverQueryEngine: The configured query engine.\n",
    "    \"\"\"\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "\n",
    "    reranker = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    auto_merging_engine = RetrieverQueryEngine(\n",
    "        retriever=retriever, node_postprocessors=[reranker]\n",
    "    )\n",
    "\n",
    "    return auto_merging_engine\n"
   ],
   "id": "e46ba6009dd1c1a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "automerging_index = build_automerging_index(\n",
    "    documents=document,\n",
    "    llm=llm,\n",
    "    embed_model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\"\n",
    ")"
   ],
   "id": "4299b4135c38cac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "automerging_query_engine = get_automerging_query_engine(\n",
    "    automerging_index,\n",
    ")"
   ],
   "id": "b80e22a421b8eb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "auto_merging_response = automerging_query_engine.query(\n",
    "    \"How does the QualityOfRecommendations setting impact the behavior of the scheduling algorithm? Describe at least two distinct ways a 'high quality' setting can alter the final recommended plan compared to a 'low quality' setting.\"\n",
    ")\n",
    "print(str(auto_merging_response))"
   ],
   "id": "c2f0a663355b66ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Llamaindex and Mongo\n",
    "## Sentence Window"
   ],
   "id": "276eb3639c54217c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:12:55.695020Z",
     "start_time": "2025-08-12T07:12:52.368505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pymongo\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Document,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "mongo_uri=\"mongodb://localhost:53888/?directConnection=true\"\n"
   ],
   "id": "e6f03f212107e73c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:12:55.704303Z",
     "start_time": "2025-08-12T07:12:55.699477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_sentence_window_index(\n",
    "    document: Document,\n",
    "    mongo_uri: str = \"mongodb://localhost:53888/?directConnection=true\",\n",
    "    db_name: str = \"my_rag_db\",\n",
    "    collection_name: str = \"sentence_vectors\",\n",
    "    index_name: str = \"vector_search_index\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a VectorStoreIndex with a sentence window node parser,\n",
    "    using MongoDB as the vector store.\n",
    "\n",
    "    Args:\n",
    "        document (Document): The document to index.\n",
    "        mongo_uri (str): The MongoDB connection string from your local Atlas deployment.\n",
    "        db_name (str): The name of the database.\n",
    "        collection_name (str): The name of the collection for vectors.\n",
    "        index_name (str): The name for the vector search index in MongoDB.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The created or loaded index.\n",
    "    \"\"\"\n",
    "    # 1. Connect to MongoDB\n",
    "    print(f\"Connecting to MongoDB at {mongo_uri}...\")\n",
    "    mongo_client = pymongo.MongoClient(mongo_uri)\n",
    "    db = mongo_client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # 2. Create the vector store object\n",
    "    vector_store = MongoDBAtlasVectorSearch(\n",
    "        mongodb_client=mongo_client,\n",
    "        db_name=db_name,\n",
    "        collection_name=collection_name,\n",
    "        vector_index_name=index_name\n",
    "    )\n",
    "\n",
    "    # 3. Create the vector search index if it doesn't exist.\n",
    "    try:\n",
    "        existing_indexes = [index['name'] for index in collection.list_search_indexes()]\n",
    "        if index_name not in existing_indexes:\n",
    "            print(f\"Vector search index '{index_name}' not found. Creating a new one...\")\n",
    "            # FIX: Get embedding dimension reliably from the model\n",
    "            # by creating a dummy embedding.\n",
    "            embed_dim = len(Settings.embed_model.get_text_embedding(\"test\"))\n",
    "            vector_store.create_vector_search_index(\n",
    "                dimensions=embed_dim,\n",
    "                path=\"embedding\", # The field where vectors are stored\n",
    "                similarity=\"cosine\"\n",
    "            )\n",
    "            print(\"Vector search index created successfully.\")\n",
    "        else:\n",
    "            print(f\"Vector search index '{index_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking or creating the search index: {e}\")\n",
    "        print(\"Please ensure you are connected to a MongoDB Atlas instance or a local Atlas deployment (started with 'atlas dev deployments start').\")\n",
    "\n",
    "\n",
    "    # 4. Create the StorageContext\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # 5. Create the node parser\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=3,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "\n",
    "    # 6. FIX: Check if documents need to be ingested by counting documents\n",
    "    # in the collection, which avoids the NotImplementedError.\n",
    "    if collection.count_documents({}) == 0:\n",
    "        print(f\"No documents found. Building new index in MongoDB collection '{collection_name}'...\")\n",
    "        nodes = node_parser.get_nodes_from_documents([document])\n",
    "        sentence_index = VectorStoreIndex(\n",
    "            nodes,\n",
    "            storage_context=storage_context,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Loading existing index from MongoDB collection '{collection_name}'...\")\n",
    "        sentence_index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "        )\n",
    "\n",
    "    print(f\"Building new index in MongoDB collection '{collection_name}' is finished.\")\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index: VectorStoreIndex,\n",
    "    similarity_top_k: int = 6,\n",
    "    rerank_top_n: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a query engine from a sentence window index with postprocessors.\n",
    "    This function does not need to change, as it is independent of the\n",
    "    storage backend.\n",
    "\n",
    "    Args:\n",
    "        sentence_index (VectorStoreIndex): The index to query.\n",
    "        similarity_top_k (int): The number of top similar results to retrieve.\n",
    "        rerank_top_n (int): The number of results to return after reranking.\n",
    "\n",
    "    Returns:\n",
    "        The query engine.\n",
    "    \"\"\"\n",
    "    # Define postprocessors to enhance retrieval\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    # Create the query engine from the index\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        node_postprocessors=[postproc, rerank],\n",
    "    )\n",
    "    return sentence_window_engine\n"
   ],
   "id": "3b39013292a68379",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:13:04.823395Z",
     "start_time": "2025-08-12T07:13:04.703565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence_index = build_sentence_window_index(\n",
    "    document=document,\n",
    "    mongo_uri=\"mongodb://localhost:53888/?directConnection=true\",\n",
    "    db_name=\"rag_db\",\n",
    "    collection_name=\"rec_sys_overviews\",\n",
    "    index_name=\"rec_sys_overviews\",\n",
    ")"
   ],
   "id": "928fc2cbb99a7904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MongoDB at mongodb://localhost:53888/?directConnection=true...\n",
      "Vector search index 'rec_sys_overviews' already exists.\n",
      "Loading existing index from MongoDB collection 'rec_sys_overviews'...\n",
      "Building new index in MongoDB collection 'rec_sys_overviews' is finished.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:13:12.682815Z",
     "start_time": "2025-08-12T07:13:09.975306Z"
    }
   },
   "cell_type": "code",
   "source": "query_engine = get_sentence_window_query_engine(sentence_index)",
   "id": "44f2353eeeb8d9b1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:13:23.956027Z",
     "start_time": "2025-08-12T07:13:19.458983Z"
    }
   },
   "cell_type": "code",
   "source": "response = query_engine.query(\"What is the primary purpose of the QuantitiesRecommendationService, and how does its output directly influence the final schedule generated by the main RecommendationService?\")",
   "id": "626c5d3740b3cf56",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:13:29.143825Z",
     "start_time": "2025-08-12T07:13:29.140597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Source Nodes ---\")\n",
    "for node in response.source_nodes:\n",
    "    # Note: The score from SentenceTransformerRerank is a relevance score, not a similarity score.\n",
    "    # Higher is better.\n",
    "    print(f\"Relevance Score: {node.score:.4f}\")\n",
    "    print(f\"Text: {node.text}\\n\")"
   ],
   "id": "afd2c1a9d7eb3731",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Source Nodes ---\n",
      "Relevance Score: 0.9961\n",
      "Text: This method likely doesn’t appear in output or user documentation except to note that\n",
      "recommendations are saved, which is usually an implementation detail but useful to mention\n",
      "that the system does keep a record.\n",
      " With these methods documented, the RecommendationService’s behavior from start to finish becomes\n",
      "clear:  it  loads  data,  processes  it,  schedules  tasks  by  finding  earliest  start  times  and  selecting  the\n",
      "appropriate workers, iterating until the full plan is built, and finally stores the outcome and returns it for\n",
      "the user .\n",
      " The careful selection and scheduling logic ensures the recommended plan is realistic, efficient, and\n",
      "leverages  the  available  workforce  effectively  while  respecting  project  constraints  and  any  pre-set\n",
      "requirements.\n",
      " QuantitiesRecommendationService\n",
      "Role: The QuantitiesRecommendationService analyzes historical and current project data to suggest\n",
      "how much work should be planned for each task type in future or ongoing projects.  It focuses on\n",
      "calculating planned quantities for tasks (e.g., volumes, lengths, or counts of work units) using statistical\n",
      "methods, so that the recommendations are grounded in data-driven estimates rather than guesses.\n",
      " This service typically runs separately (for example, as a batch job or prior step) to update a repository of\n",
      "recommended quantities.  The RecommendationService then uses those recommendations when filling\n",
      "in task details.\n",
      "\n",
      "\n",
      "Relevance Score: 0.9944\n",
      "Text: The system is organized into distinct components, each responsible for a part of the process.  Below is\n",
      "an overview of each major component (with links to detailed documentation):\n",
      "RecommendationService – The central service that orchestrates the recommendation\n",
      "generation.  It collects data, calls helper modules, and produces the final Recommendations\n",
      "output.\n",
      " QuantitiesRecommendationService – A service that analyzes historical work data to suggest\n",
      "how much work (quantity) should be planned for certain tasks.  It updates the “Recommended\n",
      "Quantities” data that the RecommendationService uses when scheduling tasks.\n",
      " Helper Utilities: Specialized static classes that perform calculations or transformations:\n",
      "VirtualWorkersHelper – Handles addition of virtual (hypothetical) workers into the pool based\n",
      "on quality settings, merging them with real worker data.\n",
      " WorkerOnBargeHelper – Processes raw worker availability (on-barge dates) into a structured\n",
      "form (continuous availability intervals for each worker).\n",
      "\n",
      "\n",
      "Relevance Score: 0.9902\n",
      "Text: Below is\n",
      "an overview of each major component (with links to detailed documentation):\n",
      "RecommendationService – The central service that orchestrates the recommendation\n",
      "generation.  It collects data, calls helper modules, and produces the final Recommendations\n",
      "output.\n",
      " QuantitiesRecommendationService – A service that analyzes historical work data to suggest\n",
      "how much work (quantity) should be planned for certain tasks.  It updates the “Recommended\n",
      "Quantities” data that the RecommendationService uses when scheduling tasks.\n",
      " Helper Utilities: Specialized static classes that perform calculations or transformations:\n",
      "VirtualWorkersHelper – Handles addition of virtual (hypothetical) workers into the pool based\n",
      "on quality settings, merging them with real worker data.\n",
      " WorkerOnBargeHelper – Processes raw worker availability (on-barge dates) into a structured\n",
      "form (continuous availability intervals for each worker).\n",
      " RecommendedDurationHelper – Calculates how long tasks could take under different\n",
      "workforce sizes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:13:34.150965Z",
     "start_time": "2025-08-12T07:13:34.147517Z"
    }
   },
   "cell_type": "code",
   "source": "response.response",
   "id": "496fd74cc6c3c0d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The primary purpose of the QuantitiesRecommendationService is to analyze historical and current project data to determine the appropriate planned quantities for various task types in future or ongoing projects. It utilizes statistical methods to provide data-driven estimates of how much work should be planned, such as volumes, lengths, or counts of work units.\\n\\nThe output from the QuantitiesRecommendationService directly influences the final schedule generated by the RecommendationService by updating the \"Recommended Quantities\" data. This data is then utilized by the RecommendationService when scheduling tasks, ensuring that the recommendations are based on accurate and relevant information, which contributes to a realistic and efficient project plan.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
